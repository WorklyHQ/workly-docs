# âš¡ LLM Cache Optimization - Desktop-Mate Session 11

**Date** : 28 octobre 2025  
**Version** : v0.12.0-alpha  
**Objectif** : RÃ©duire la latence de la premiÃ¨re gÃ©nÃ©ration et optimiser le cache LLM

---

## ğŸ¯ Objectifs

1. âœ… Mesurer latences baseline (cold vs warm cache)
2. âœ… Identifier impact paramÃ¨tres (`n_ctx`, `n_batch`, `use_mlock`)
3. ğŸ”„ ImplÃ©menter warming cache au dÃ©marrage
4. ğŸ“ˆ RÃ©duire latence premiÃ¨re gÃ©nÃ©ration de -20-30%

---

## ğŸ“Š Baseline Phase 1

**Observations Memory Profiling** :
- RAM premiÃ¨re gÃ©nÃ©ration : **+433 MB** ğŸ”´
- RAM deuxiÃ¨me gÃ©nÃ©ration : **+0.57 MB** âœ…
- **Ratio** : 433 MB / 0.57 MB = **760:1** (premiÃ¨re gÃ©nÃ©ration 760x plus coÃ»teuse !)

**ProblÃ¨me identifiÃ©** :
Le cache KV (Key-Value) est allouÃ© dynamiquement lors de la premiÃ¨re gÃ©nÃ©ration, causant :
- Latence Ã©levÃ©e
- Augmentation RAM importante
- Mauvaise expÃ©rience utilisateur (attente)

---

## ğŸ› ï¸ Outils & Scripts

### Script Principal : `scripts/benchmark_llm.py`

**Benchmarks disponibles** :
1. **Cold start** - Chargement modÃ¨le + premiÃ¨re gÃ©nÃ©ration
2. **Warm cache** - 10 gÃ©nÃ©rations consÃ©cutives (statistiques)
3. **Context sizes** - Impact taille prompt (court/moyen/long)
4. **Max tokens** - Impact `max_tokens` sur vitesse (25/50/100/150)
5. **Tous les benchmarks** - SÃ©quence complÃ¨te

**Usage** :
```powershell
# Activer venv
.\venv\Scripts\Activate.ps1

# Lancer un benchmark spÃ©cifique
python scripts/benchmark_llm.py 1  # Cold start

# Lancer tous les benchmarks
python scripts/benchmark_llm.py 5
```

**Outputs** :
- Console : MÃ©triques en temps rÃ©el
- Fichier : `llm_benchmark_results.txt`

---

## ğŸ“Š RÃ©sultats - Benchmark 1 : Cold Start

### Mesures ComplÃ¨tes âœ…

| MÃ©trique | Valeur | Notes |
|----------|--------|-------|
| **Temps chargement modÃ¨le** | **5.60s** | Chargement Zephyr-7B sur GPU |
| **Temps premiÃ¨re gÃ©nÃ©ration** | **2.13s** | Cold cache (allocation KV) |
| **Tokens gÃ©nÃ©rÃ©s** | **41** | Prompt: "Bonjour, comment vas-tu ?" |
| **Vitesse** | **19.27 tokens/sec** | Baseline cold start |

### Analyse DÃ©taillÃ©e

**Temps chargement modÃ¨le : 5.60s** âœ…
- Profil "performance" optimal
- Charge 4.2 GB modÃ¨le en VRAM (~5.2 GB rÃ©el avec overhead)
- Initialise tensors, buffers, cache structures
- **CohÃ©rent** avec attentes (5-10s)

**Temps premiÃ¨re gÃ©nÃ©ration : 2.13s** âœ…
- Cold cache : allocation dynamique cache KV
- Impact RAM : +433 MB (observÃ© Phase 1)
- **CohÃ©rent** avec baseline Phase 1 (2-5s)
- Cause : Initialisation cache, premiers tensors

**Vitesse : 19.27 tokens/sec** ğŸŸ¡
- **InfÃ©rieur** Ã  baseline Chat 9 (25-35 tok/s)
- **Explication** : Prompt court (41 tokens gÃ©nÃ©rÃ©s)
- Overhead initialisation proportionnellement plus Ã©levÃ©

---

## ğŸ“Š RÃ©sultats - Benchmark 2 : Warm Cache

### Mesures ComplÃ¨tes âœ… (10 runs)

| MÃ©trique | Valeur | Notes |
|----------|--------|-------|
| **Latence moyenne** | **1.754s** | Moyenne sur 10 gÃ©nÃ©rations |
| **Latence mÃ©diane** | **1.760s** | Valeur mÃ©diane |
| **Latence min / max** | **1.728s / 1.771s** | Plage (variance: 0.043s) |
| **Ã‰cart-type** | **0.014s** | TrÃ¨s stable âœ… |
| **Vitesse moyenne** | **18.08 tokens/sec** | Moyenne sur 10 runs |
| **Vitesse mÃ©diane** | **18.48 tokens/sec** | Valeur mÃ©diane |

### Analyse DÃ©taillÃ©e

**Latence warm cache : 1.754s** âœ…
- **AmÃ©lioration vs cold** : **-17.6%** (2.13s â†’ 1.75s)
- Cache KV rÃ©utilisÃ© (pas de rÃ©allocation)
- **Plus rapide que cold** comme attendu

**StabilitÃ© excellente : Ã©cart-type 0.014s** ğŸ‰
- Variance trÃ¨s faible (Â±0.8% de la moyenne)
- **Performance constante** entre runs
- Pas d'impact charges systÃ¨me externes
- **PrÃ©dictibilitÃ©** : expÃ©rience utilisateur stable

**Vitesse gÃ©nÃ©ration : 18.08 tokens/sec** ğŸŸ¡
- **InfÃ©rieur** Ã  baseline Chat 9 (25-35 tok/s)
- **Possible explication** :
  - Prompts courts utilisÃ©s dans benchmark (5-20 mots)
  - Overhead proportionnellement plus Ã©levÃ©
  - GÃ©nÃ©ration limitÃ©e Ã  50 tokens max
  
**Comparaison Cold vs Warm** :
| Type | Latence | AmÃ©lioration |
|------|---------|--------------|
| Cold start | **2.13s** | - |
| Warm cache | **1.75s** | **-17.6%** âœ… |

**Conclusion Benchmark 2** :
- âœ… Cache warm fonctionne correctement
- âœ… StabilitÃ© excellente (variance <1%)
- ğŸ” Vitesse infÃ©rieure attentes (Ã  investiguer Benchmark 4)

---

## ğŸ“Š RÃ©sultats - Benchmark 3 : Impact Taille Contexte

### Mesures ComplÃ¨tes âœ…

| Contexte | Taille Prompt | Latence Moyenne | DiffÃ©rence vs Court | Notes |
|----------|---------------|-----------------|---------------------|-------|
| **Court** | **1 mot** | **1.731s** | - | "Bonjour" |
| **Moyen** | **9 mots** | **1.795s** | **+0.064s (+3.7%)** | Phrase complÃ¨te |
| **Long** | **35 mots** | **1.855s** | **+0.124s (+7.2%)** | Paragraphe |

### Analyse DÃ©taillÃ©e

**Pattern observÃ© : Augmentation linÃ©aire** âœ…
- Court â†’ Moyen (+8 mots) : **+0.064s**
- Moyen â†’ Long (+26 mots) : **+0.060s**
- **Impact moyen** : ~0.002-0.003s par mot additionnel

**Overhead prompt processing** :
- Petit contexte (1-10 mots) : impact faible (~3%)
- Grand contexte (30-50 mots) : impact modÃ©rÃ© (~7%)
- **Acceptable** pour usage Desktop-Mate

**Formule approximative** :
```
Latence â‰ˆ 1.73s + (0.0024s Ã— nb_mots_prompt)
```

**Conclusion Benchmark 3** :
- âœ… Impact taille contexte prÃ©visible et linÃ©aire
- âœ… Overhead faible (<10% mÃªme pour 35 mots)
- âœ… Pas d'optimisation nÃ©cessaire sur ce front

---

## ğŸ“Š RÃ©sultats - Benchmark 4 : Impact Max Tokens

### Mesures ComplÃ¨tes âœ…

| Max Tokens | Latence Moyenne | Tokens GÃ©nÃ©rÃ©s | Vitesse (tok/s) | DiffÃ©rence | Notes |
|------------|-----------------|----------------|-----------------|------------|-------|
| **25** | **0.874s** | **14.3** | **16.40 tok/s** | - | RÃ©ponse courte |
| **50** | **1.736s** | **29.3** | **16.90 tok/s** | **+0.86s** | Baseline |
| **100** | **3.485s** | **55.0** | **15.78 tok/s** | **+2.61s** | RÃ©ponse longue |
| **150** | **4.702s** | **75.0** | **15.95 tok/s** | **+3.83s** | RÃ©ponse trÃ¨s longue |

### Analyse DÃ©taillÃ©e

**Pattern observÃ© : Latence augmente linÃ©airement** âœ…
- 25 â†’ 50 tokens (+25) : **+0.86s** â†’ ~0.034s/token
- 50 â†’ 100 tokens (+50) : **+1.75s** â†’ ~0.035s/token
- 100 â†’ 150 tokens (+50) : **+1.22s** â†’ ~0.024s/token

**Vitesse gÃ©nÃ©ration constante : 15-17 tok/s** âœ…
- Variance faible (16.40 â†’ 15.78 â†’ 15.95)
- **StabilitÃ©** confirmÃ©e
- **CohÃ©rent** avec Benchmark 2 (18.08 tok/s)

**Formule validÃ©e** :
```
Latence â‰ˆ overhead_fixe + (tokens_gÃ©nÃ©rÃ©s / vitesse)
Latence â‰ˆ 0.5s + (tokens / 16 tok/s)
```

**Exemple** : GÃ©nÃ©rer 100 tokens
- ThÃ©orique : 0.5 + (100/16) = **6.75s**
- MesurÃ© : **3.49s** pour 55 tokens gÃ©nÃ©rÃ©s
- **Note** : Le modÃ¨le gÃ©nÃ¨re moins que `max_tokens` (55 vs 100), car il s'arrÃªte naturellement (EOS token)

**Pourquoi vitesse < baseline Chat 9 (25-35 tok/s) ?** ğŸ”
- Benchmark utilise **prompt court** ("Raconte-moi une courte histoire")
- Chat 9 mesurÃ© avec **conversations rÃ©elles** (contexte plus riche)
- **HypothÃ¨se** : Contexte conversationnel booste vitesse gÃ©nÃ©ration
- **Validation nÃ©cessaire** : Tester avec contexte conversationnel complet

**Conclusion Benchmark 4** :
- âœ… Latence prÃ©visible et linÃ©aire avec max_tokens
- âœ… Vitesse stable (~16 tok/s pour prompts courts)
- ğŸ” DiffÃ©rence avec baseline Chat 9 Ã  investiguer (contexte conversationnel)

---

## ğŸ”¬ ParamÃ¨tres LLM Actuels (Profil "performance")

```python
{
    "n_gpu_layers": -1,        # Toutes les layers sur GPU
    "n_ctx": 4096,            # Context window (tokens)
    "n_batch": 512,           # Batch size processing
    "n_threads": 6,           # CPU threads
    "use_mlock": True,        # Lock memory (Ã©vite swap)
    "verbose": False
}
```

### Impact ParamÃ¨tres

**`n_ctx` (Context Window)** :
- Valeur actuelle : **4096** tokens
- Impact : Taille maximale cache KV
- **Trade-off** : Plus grand = plus de mÃ©moire, mais contexte plus long

**`n_batch` (Batch Size)** :
- Valeur actuelle : **512**
- Impact : Nombre tokens traitÃ©s simultanÃ©ment
- **Trade-off** : Plus grand = plus rapide, mais plus de VRAM

**`use_mlock` (Memory Lock)** :
- Valeur actuelle : **True**
- Impact : EmpÃªche swap vers disque (garde tout en RAM/VRAM)
- **Avantage** : Latence stable, pas de ralentissements

---

## ğŸ¯ Optimisations Ã  Tester

### 1. Warming Cache au DÃ©marrage

**Concept** : PrÃ©-gÃ©nÃ©rer 1-2 tokens lors de `load_model()` pour allouer le cache KV

**ImplÃ©mentation** :
```python
# Dans ModelManager.load_model()
def load_model(self):
    # ... chargement modÃ¨le ...
    
    # Warming cache (optionnel)
    if warm_cache:
        logger.info("ğŸ”¥ Warming cache...")
        self.generate("Bonjour", max_tokens=2)
        logger.info("âœ… Cache warmed")
```

**Avantages** :
- âœ… PremiÃ¨re gÃ©nÃ©ration utilisateur plus rapide
- âœ… Cache KV dÃ©jÃ  allouÃ©
- âœ… Meilleure UX (pas d'attente premiÃ¨re rÃ©ponse)

**InconvÃ©nients** :
- âŒ Temps chargement modÃ¨le augmente de +1-2s
- âŒ RAM utilisÃ©e immÃ©diatement (+433 MB)

**Trade-off** :
- Acceptable si chargement au dÃ©marrage app (une seule fois)
- ProblÃ©matique si chargement Ã  la demande

### 2. Ajuster `n_ctx` selon Usage

**Concept** : RÃ©duire context window si conversations courtes

**Options** :
- **4096** (actuel) : Conversations longues (~3000 mots historique)
- **2048** : Conversations moyennes (~1500 mots)
- **1024** : Conversations courtes (~750 mots)

**Impact attendu** :
- `n_ctx` rÃ©duit â†’ RAM/VRAM cache rÃ©duite
- Trade-off : Contexte conversationnel plus limitÃ©

**Recommandation** :
- Garder **4096** par dÃ©faut (optimal pour Desktop-Mate)
- Proposer option dans GUI si besoin

### 3. Ajuster `n_batch` selon GPU

**Concept** : Optimiser batch size selon VRAM disponible

**Options** :
- **512** (actuel) : RTX 4050 6GB (optimal)
- **256** : GPU 4GB
- **1024** : GPU 8GB+

**Impact attendu** :
- Batch plus grand â†’ GÃ©nÃ©ration plus rapide
- Batch trop grand â†’ Out of memory

**Recommandation** :
- Garder **512** pour RTX 4050 6GB
- Auto-dÃ©tection selon VRAM (Phase 5 - GPU Profiling)

---

## ğŸ§ª Tests Ã  Effectuer

### Test 1 : Warming Cache

**HypothÃ¨se** : RÃ©duction latence premiÃ¨re gÃ©nÃ©ration utilisateur de -50%

**MÃ©thode** :
1. Baseline : Mesurer latence premiÃ¨re gÃ©nÃ©ration sans warming
2. Test : Activer warming au chargement
3. Mesurer latence premiÃ¨re gÃ©nÃ©ration utilisateur
4. Comparer diffÃ©rence

**MÃ©triques** :
- Latence baseline : ? s
- Latence avec warming : ? s
- AmÃ©lioration : ? %

### Test 2 : Impact n_ctx

**HypothÃ¨se** : RÃ©duction RAM/VRAM proportionnelle Ã  n_ctx

**MÃ©thode** :
1. Tester n_ctx = 1024, 2048, 4096
2. Mesurer VRAM utilisÃ©e aprÃ¨s chargement
3. Mesurer RAM premiÃ¨re gÃ©nÃ©ration
4. Comparer vitesse gÃ©nÃ©ration

**MÃ©triques** :
| n_ctx | VRAM (MB) | RAM 1Ã¨re gen (MB) | Vitesse (tok/s) |
|-------|-----------|-------------------|-----------------|
| 1024 | ? | ? | ? |
| 2048 | ? | ? | ? |
| 4096 | ? | ? | ? |

### Test 3 : Impact n_batch

**HypothÃ¨se** : Batch plus grand â†’ Vitesse plus rapide (jusqu'Ã  limite VRAM)

**MÃ©thode** :
1. Tester n_batch = 256, 512, 1024
2. Mesurer vitesse gÃ©nÃ©ration
3. Monitorer VRAM usage
4. Identifier optimal

**MÃ©triques** :
| n_batch | Vitesse (tok/s) | VRAM (MB) | Stable ? |
|---------|-----------------|-----------|----------|
| 256 | ? | ? | ? |
| 512 | ? | ? | ? |
| 1024 | ? | ? | ? |

---

## âš ï¸ Points d'Attention

### 1. Trade-off Warming vs Temps Chargement

**Warming cache** est une optimisation Ã  **double tranchant** :
- âœ… AmÃ©liore UX premiÃ¨re gÃ©nÃ©ration
- âŒ Ralentit chargement initial

**Recommandation** :
- Activer warming si chargement au **dÃ©marrage app**
- DÃ©sactiver warming si chargement **Ã  la demande**

### 2. VRAM LimitÃ©e (6 GB)

Avec RTX 4050 6GB, marges limitÃ©es :
- ModÃ¨le : ~5.2 GB VRAM
- Cache : ~0.8 GB VRAM
- **Total** : ~6.0 GB (limite atteinte !)

**Attention** :
- Augmenter `n_ctx` ou `n_batch` risque **Out of Memory**
- Tester prudemment avec monitoring VRAM

### 3. CohÃ©rence Benchmarks

**Facteurs externes** peuvent impacter :
- Charge CPU/GPU systÃ¨me
- TempÃ©rature GPU (throttling)
- Processus concurrents

**Recommandation** :
- Effectuer 3-5 runs par test
- Prendre mÃ©diane (plus stable que moyenne)
- RÃ©pÃ©ter si variance Ã©levÃ©e

---

## ğŸ“š Ressources

### Documentation llama-cpp-python
- [ParamÃ¨tres gÃ©nÃ©ration](https://github.com/abetlen/llama-cpp-python#generation-parameters)
- [Cache optimization](https://github.com/ggerganov/llama.cpp/discussions/2094)
- [CUDA backend tuning](https://github.com/ggerganov/llama.cpp/blob/master/docs/backend/CUDA.md)

### Benchmarks CommunautÃ©
- [Zephyr-7B performance](https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF)
- [llama.cpp benchmarks](https://github.com/ggerganov/llama.cpp/discussions/1509)

---

## âœ… Checklist Phase 2

- [x] **CrÃ©er script benchmarking** (`scripts/benchmark_llm.py`)
- [x] **ExÃ©cuter benchmarks baseline** (cold/warm/contexte/max_tokens) âœ…
- [x] **Analyser rÃ©sultats baseline** âœ…
- [x] **ImplÃ©menter warming cache** âœ… (ModelManager.load_model(warm_cache=True))
- [x] **Tester warming cache** (benchmark avant/aprÃ¨s) âœ…
- [ ] **Documenter rÃ©sultats finaux** â³ En cours
- [ ] **Archiver scripts** dans `docs/sessions/session_11_performance/scripts/`

---

## ğŸ‰ RÃ©sultats - Test Warming Cache

### ImplÃ©mentation

**Modification** : `src/ai/model_manager.py`
```python
def load_model(self, force_profile=None, warm_cache=True):
    # ... chargement modÃ¨le ...
    
    # Warming cache si demandÃ©
    if warm_cache:
        logger.info("ğŸ”¥ Warming cache (prÃ©-allocation KV)...")
        _ = self.generate(prompt="Bonjour", max_tokens=2, temperature=0.0)
        logger.info("âœ… Cache warmed")
```

**ParamÃ¨tre** : `warm_cache=True` par dÃ©faut (activÃ©)

### Comparaison Avant/AprÃ¨s âœ…

| MÃ©trique | Sans Warming | Avec Warming | AmÃ©lioration |
|----------|--------------|--------------|--------------|
| **Temps chargement** | **5.10s** | **2.57s** | **-49.7%** ğŸ‰ |
| **1Ã¨re gÃ©nÃ©ration** | **2.11s** | **1.75s** | **-16.9%** âœ… |
| **Vitesse 1Ã¨re gen** | 19.46 tok/s | 22.28 tok/s | **+14%** âœ… |
| **2Ã¨me gÃ©nÃ©ration** | 1.76s | 1.76s | 0% (identique) |

### Analyse DÃ©taillÃ©e

#### ğŸ‰ DÃ©couverte Surprenante : Chargement Plus Rapide !

**Observation** : Le chargement AVEC warming est **2.5x plus rapide** (5.10s â†’ 2.57s) !

**Explications possibles** :
1. **Variance systÃ¨me** : Charge CPU/GPU diffÃ©rente entre les deux tests
2. **Cache disque OS** : DeuxiÃ¨me chargement bÃ©nÃ©ficie cache systÃ¨me
3. **Optimisations llama.cpp** : Pre-allocation cache optimise init interne

**âš ï¸ Note** : Ce rÃ©sultat doit Ãªtre validÃ© avec **multiple runs** pour confirmer

#### âœ… AmÃ©lioration PremiÃ¨re GÃ©nÃ©ration : -16.9%

**Sans warming** :
- Latence : **2.11s**
- Allocation cache KV dynamique
- Impact RAM : +433 MB (Phase 1)

**Avec warming** :
- Latence : **1.75s** (-0.36s)
- Cache KV prÃ©-allouÃ© au chargement
- PremiÃ¨re gÃ©nÃ©ration utilisateur immÃ©diate

**Trade-off** :
- CoÃ»t warming : ~2s (gÃ©nÃ©ration 2 tokens)
- Gain utilisateur : -0.36s (17%)
- **Net** : Positif car chargement une fois seulement

#### âœ… Vitesse GÃ©nÃ©ration : +14%

**Sans warming** : 19.46 tok/s
**Avec warming** : 22.28 tok/s (+2.82 tok/s)

**Explication** :
- Cache KV prÃ©-allouÃ© â†’ Moins d'overhead init
- Tensors dÃ©jÃ  en place â†’ GÃ©nÃ©ration plus fluide

#### âœ… DeuxiÃ¨me GÃ©nÃ©ration : Identique

**Sans/Avec warming** : 1.76s (mÃªme latence)

**Validation** : AprÃ¨s premiÃ¨re gÃ©nÃ©ration, le cache est warm dans les deux cas â†’ performance identique

### Conclusion Warming Cache

**âœ… RECOMMANDATION : Activer warming par dÃ©faut**

**Avantages** :
- âœ… PremiÃ¨re gÃ©nÃ©ration **17% plus rapide**
- âœ… Vitesse gÃ©nÃ©ration **+14%**
- âœ… ExpÃ©rience utilisateur amÃ©liorÃ©e (pas d'attente)
- âœ… Chargement potentiellement plus rapide (Ã  valider)

**InconvÃ©nients** :
- âŒ Temps chargement augmente de ~2s (si variance systÃ¨me confirmÃ©e)
- âŒ RAM utilisÃ©e immÃ©diatement (+433 MB)

**Trade-off** :
- **Acceptable** si chargement au dÃ©marrage app (une fois)
- **Optimal** pour Desktop-Mate (modÃ¨le chargÃ© au launch)

**ImplÃ©mentation finale** :
```python
# Dans main.py ou GUI
model_manager.load_model(warm_cache=True)  # Par dÃ©faut
```

---

**ğŸŠ Phase 2 - LLM Cache Optimization : 90% COMPLÃ‰TÃ‰E ! Warming implÃ©mentÃ© avec succÃ¨s ! ğŸ”¥**

---

_DerniÃ¨re mise Ã  jour : 28 octobre 2025 16:30_  
_Warming cache : AmÃ©lioration -17% latence premiÃ¨re gÃ©nÃ©ration confirmÃ©e !_
