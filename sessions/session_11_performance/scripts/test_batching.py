"""
Test Batching - Compare les performances avec et sans batching

Ce script mesure l'amÃ©lioration apportÃ©e par le message batching en comparant :
- Envoi de N commandes sÃ©parÃ©es (baseline)
- Envoi de N commandes groupÃ©es en batch

Usage:
    python scripts/test_batching.py
    
PrÃ©requis:
    - Unity lancÃ© avec PythonBridge actif
"""

import sys
import os
import time
import statistics

# Ajouter le dossier racine au path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from src.ipc.unity_bridge import UnityBridge


class BatchingTest:
    """Classe pour tester l'amÃ©lioration du batching."""
    
    def __init__(self):
        """Initialise le test."""
        self.bridge = UnityBridge()
        
    def setup(self) -> bool:
        """Configure la connexion Unity."""
        print("=" * 70)
        print("ðŸ”¬ TEST BATCHING - Comparaison avant/aprÃ¨s")
        print("=" * 70)
        print()
        
        print("ðŸ“¡ Connexion Ã  Unity...")
        if not self.bridge.connect():
            print("âŒ Ã‰chec de connexion Ã  Unity !")
            return False
        
        print("âœ… ConnectÃ© Ã  Unity !")
        print()
        time.sleep(0.5)
        return True
    
    def teardown(self):
        """Nettoie les ressources."""
        print()
        print("ðŸ”Œ DÃ©connexion...")
        self.bridge.disconnect()
    
    def test_without_batching(self, n_commands: int = 100) -> dict:
        """Test SANS batching : envoi de commandes sÃ©parÃ©es.
        
        Args:
            n_commands: Nombre de commandes Ã  envoyer
            
        Returns:
            Statistiques de performance
        """
        print(f"ðŸ“Š Test 1 : SANS batching ({n_commands} commandes sÃ©parÃ©es)")
        print("-" * 70)
        
        # Warmup
        print("ðŸ”¥ Warmup...")
        for _ in range(10):
            self.bridge.send_command("test", {"id": 0})
            time.sleep(0.001)
        
        print(f"â±ï¸  Envoi de {n_commands} commandes individuelles...")
        
        latencies = []
        
        start_total = time.perf_counter()
        
        for i in range(n_commands):
            start = time.perf_counter()
            
            self.bridge.send_command("test", {"id": i})
            
            end = time.perf_counter()
            latency_ms = (end - start) * 1000
            latencies.append(latency_ms)
            
            # Petit dÃ©lai pour ne pas saturer
            time.sleep(0.001)
        
        end_total = time.perf_counter()
        total_time_ms = (end_total - start_total) * 1000
        
        # Statistiques
        stats = {
            "count": len(latencies),
            "mean_ms": statistics.mean(latencies),
            "median_ms": statistics.median(latencies),
            "total_time_ms": total_time_ms,
            "throughput_msg_per_sec": n_commands / (total_time_ms / 1000)
        }
        
        print(f"   âœ… TerminÃ© en {total_time_ms:.2f} ms")
        print()
        print("ðŸ“ˆ RÃ©sultats SANS batching :")
        print(f"   Latence moyenne      : {stats['mean_ms']:.3f} ms/commande")
        print(f"   Latence mÃ©diane      : {stats['median_ms']:.3f} ms/commande")
        print(f"   Temps total          : {stats['total_time_ms']:.2f} ms")
        print(f"   Throughput           : {stats['throughput_msg_per_sec']:.2f} msg/s")
        print()
        
        return stats
    
    def test_with_batching(self, n_commands: int = 100, batch_size: int = 10) -> dict:
        """Test AVEC batching : envoi de commandes groupÃ©es.
        
        Args:
            n_commands: Nombre total de commandes
            batch_size: Taille de chaque batch
            
        Returns:
            Statistiques de performance
        """
        print(f"ðŸ“Š Test 2 : AVEC batching ({n_commands} commandes, batches de {batch_size})")
        print("-" * 70)
        
        # Warmup
        print("ðŸ”¥ Warmup...")
        test_batch = [{"command": "test", "data": {"id": i}} for i in range(5)]
        self.bridge.send_batch(test_batch)
        time.sleep(0.1)
        
        print(f"â±ï¸  Envoi de {n_commands // batch_size} batches...")
        
        batch_latencies = []
        
        start_total = time.perf_counter()
        
        for batch_num in range(n_commands // batch_size):
            # CrÃ©er un batch
            batch = []
            for i in range(batch_size):
                cmd_id = batch_num * batch_size + i
                batch.append({
                    "command": "test",
                    "data": {"id": cmd_id}
                })
            
            # Envoyer le batch
            start = time.perf_counter()
            
            self.bridge.send_batch(batch)
            
            end = time.perf_counter()
            latency_ms = (end - start) * 1000
            batch_latencies.append(latency_ms)
            
            time.sleep(0.001)
        
        end_total = time.perf_counter()
        total_time_ms = (end_total - start_total) * 1000
        
        # Statistiques
        n_batches = len(batch_latencies)
        latency_per_command = statistics.mean(batch_latencies) / batch_size
        
        stats = {
            "count": n_commands,
            "n_batches": n_batches,
            "batch_size": batch_size,
            "mean_batch_ms": statistics.mean(batch_latencies),
            "mean_per_command_ms": latency_per_command,
            "median_batch_ms": statistics.median(batch_latencies),
            "total_time_ms": total_time_ms,
            "throughput_msg_per_sec": n_commands / (total_time_ms / 1000)
        }
        
        print(f"   âœ… TerminÃ© en {total_time_ms:.2f} ms")
        print()
        print("ðŸ“ˆ RÃ©sultats AVEC batching :")
        print(f"   Batches envoyÃ©s      : {stats['n_batches']}")
        print(f"   Latence par batch    : {stats['mean_batch_ms']:.3f} ms")
        print(f"   Latence par commande : {stats['mean_per_command_ms']:.3f} ms")
        print(f"   Temps total          : {stats['total_time_ms']:.2f} ms")
        print(f"   Throughput           : {stats['throughput_msg_per_sec']:.2f} msg/s")
        print()
        
        return stats
    
    def compare_results(self, without: dict, with_batching: dict):
        """Compare les rÃ©sultats et calcule l'amÃ©lioration."""
        print("=" * 70)
        print("ðŸ† COMPARAISON FINALE")
        print("=" * 70)
        print()
        
        # Comparaison latence par commande
        latency_without = without["mean_ms"]
        latency_with = with_batching["mean_per_command_ms"]
        latency_improvement = ((latency_without - latency_with) / latency_without) * 100
        
        # Comparaison temps total
        time_without = without["total_time_ms"]
        time_with = with_batching["total_time_ms"]
        time_improvement = ((time_without - time_with) / time_without) * 100
        
        # Comparaison throughput
        throughput_without = without["throughput_msg_per_sec"]
        throughput_with = with_batching["throughput_msg_per_sec"]
        throughput_improvement = ((throughput_with - throughput_without) / throughput_without) * 100
        
        print("ðŸ“Š Latence par commande :")
        print(f"   SANS batching  : {latency_without:.3f} ms")
        print(f"   AVEC batching  : {latency_with:.3f} ms")
        
        if latency_improvement > 0:
            print(f"   âœ… AmÃ©lioration : -{latency_improvement:.1f}% (plus rapide) ðŸš€")
        else:
            print(f"   âš ï¸  RÃ©gression  : +{abs(latency_improvement):.1f}% (plus lent)")
        print()
        
        print("â±ï¸  Temps total :")
        print(f"   SANS batching  : {time_without:.2f} ms")
        print(f"   AVEC batching  : {time_with:.2f} ms")
        
        if time_improvement > 0:
            print(f"   âœ… AmÃ©lioration : -{time_improvement:.1f}% (plus rapide) ðŸš€")
        else:
            print(f"   âš ï¸  RÃ©gression  : +{abs(time_improvement):.1f}% (plus lent)")
        print()
        
        print("ðŸš€ Throughput :")
        print(f"   SANS batching  : {throughput_without:.2f} msg/s")
        print(f"   AVEC batching  : {throughput_with:.2f} msg/s")
        
        if throughput_improvement > 0:
            print(f"   âœ… AmÃ©lioration : +{throughput_improvement:.1f}% (plus rapide) ðŸš€")
        else:
            print(f"   âš ï¸  RÃ©gression  : {abs(throughput_improvement):.1f}% (plus lent)")
        print()
        
        print("=" * 70)
        
        # Verdict final
        if latency_improvement > 0 and time_improvement > 0:
            print("ðŸŽ‰ VERDICT : Le batching AMÃ‰LIORE les performances ! âœ…")
        elif latency_improvement < -5 or time_improvement < -5:
            print("âš ï¸  VERDICT : Le batching DÃ‰GRADE les performances")
        else:
            print("ðŸ¤· VERDICT : AmÃ©lioration marginale, non significative")
        
        print("=" * 70)
        
        # Sauvegarder les rÃ©sultats
        self.save_comparison(without, with_batching, {
            "latency_improvement_percent": latency_improvement,
            "time_improvement_percent": time_improvement,
            "throughput_improvement_percent": throughput_improvement
        })
    
    def save_comparison(self, without: dict, with_batching: dict, improvements: dict):
        """Sauvegarde la comparaison dans un fichier."""
        filepath = os.path.join("scripts", "batching_comparison_results.txt")
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write("=" * 70 + "\n")
            f.write("ðŸ”¬ COMPARAISON BATCHING - Python â†” Unity\n")
            f.write("=" * 70 + "\n\n")
            
            f.write(f"ðŸ“… Date : {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("SANS batching (baseline) :\n")
            f.write(f"  Latence moyenne      : {without['mean_ms']:.3f} ms/commande\n")
            f.write(f"  Temps total          : {without['total_time_ms']:.2f} ms\n")
            f.write(f"  Throughput           : {without['throughput_msg_per_sec']:.2f} msg/s\n\n")
            
            f.write("AVEC batching (optimisÃ©) :\n")
            f.write(f"  Latence par commande : {with_batching['mean_per_command_ms']:.3f} ms\n")
            f.write(f"  Temps total          : {with_batching['total_time_ms']:.2f} ms\n")
            f.write(f"  Throughput           : {with_batching['throughput_msg_per_sec']:.2f} msg/s\n\n")
            
            f.write("AMÃ‰LIORATION :\n")
            f.write(f"  Latence : {improvements['latency_improvement_percent']:+.1f}%\n")
            f.write(f"  Temps   : {improvements['time_improvement_percent']:+.1f}%\n")
            f.write(f"  Throughput : {improvements['throughput_improvement_percent']:+.1f}%\n\n")
            
            f.write("=" * 70 + "\n")
        
        print(f"\nðŸ’¾ Comparaison sauvegardÃ©e dans : {filepath}")
    
    def run_comparison(self, n_commands: int = 100, batch_size: int = 10):
        """ExÃ©cute la comparaison complÃ¨te."""
        if not self.setup():
            return
        
        try:
            # Test SANS batching
            stats_without = self.test_without_batching(n_commands)
            time.sleep(1)
            
            # Test AVEC batching
            stats_with = self.test_with_batching(n_commands, batch_size)
            time.sleep(1)
            
            # Comparer
            self.compare_results(stats_without, stats_with)
            
        finally:
            self.teardown()


def main():
    """Point d'entrÃ©e principal."""
    test = BatchingTest()
    test.run_comparison(n_commands=100, batch_size=10)


if __name__ == "__main__":
    main()
