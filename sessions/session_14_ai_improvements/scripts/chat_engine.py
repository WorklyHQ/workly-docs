"""
Chat Engine pour Workly (Kira)

Moteur conversationnel unifi√© qui orchestre :
- M√©moire conversationnelle court-terme (ConversationMemory)
- M√©moire long-terme avanc√©e (MemoryManager) - Phase 1
- Personnalit√© √©volutive (PersonalityEngine) - Phase 2
- Analyse √©motionnelle avanc√©e (EmotionAnalyzer) - Phase 3
- G√©n√©ration LLM (ModelManager)
- Construction prompts avec contexte
- Sauvegarde automatique des conversations

Phases IA :
- Phase 1 : M√©moire long-terme (r√©sum√©s, faits, recherche s√©mantique)
- Phase 2 : Personnalit√© √©volutive (6 traits, adaptation contexte)
- Phase 3 : √âmotions avanc√©es (compos√©es, m√©moire √©motionnelle, transitions)
"""

import logging
import os
from typing import Optional, Dict, List, Any
from dataclasses import dataclass

from .memory import ConversationMemory, get_memory
from .model_manager import ModelManager, get_model_manager
from .config import AIConfig, get_config
from .memory_manager import MemoryManager
from .personality_engine import PersonalityEngine
from .emotion_analyzer import EmotionAnalyzer

logger = logging.getLogger(__name__)


@dataclass
class ChatResponse:
    """R√©ponse du chat engine"""

    response: str  # Texte g√©n√©r√© par le mod√®le
    emotion: str  # √âmotion d√©tect√©e ('joy', 'angry', etc.)
    tokens_used: int  # Nombre approximatif de tokens
    context_messages: int  # Nombre de messages dans le contexte
    processing_time: float  # Temps de traitement en secondes


# EmotionDetector supprim√© - remplac√© par EmotionAnalyzer (Phase 3)


class ChatEngine:
    """
    D√©tecteur d'√©motions basique par mots-cl√©s

    Analyse le texte g√©n√©r√© et retourne l'√©motion dominante.
    Version simple mais efficace pour Workly.
    """

    # Mots-cl√©s par √©motion (fran√ßais)
    EMOTION_KEYWORDS = {
        "joy": [
            "heureux",
            "heureuse",
            "content",
            "contente",
            "super",
            "g√©nial",
            "excellent",
            "parfait",
            "cool",
            "top",
            "joie",
            "merveilleux",
            "üòä",
            "üòÑ",
            "üòÅ",
            "üéâ",
            "‚ú®",
            "ü•∞",
            "üòç",
            "ü§ó",
            "r√©joui",
            "enchant√©",
            "ravi",
            "formidable",
            "magnifique",
        ],
        "angry": [
            "√©nerv√©",
            "√©nerv√©e",
            "col√®re",
            "furieux",
            "furieuse",
            "agac√©",
            "irrit√©",
            "f√¢ch√©",
            "rage",
            "m√©content",
            "contrari√©",
            "aga√ßant",
            "üò†",
            "üò°",
            "ü§¨",
            "grrr",
            "argh",
            "pfff",
            "exasp√©r√©",
            "frustr√©",
            "indign√©",
            "erreur",
            "probl√®me",
        ],
        "sorrow": [
            "triste",
            "d√©sol√©",
            "d√©sol√©e",
            "dommage",
            "malheureusement",
            "h√©las",
            "peine",
            "chagrin",
            "malheureux",
            "m√©lancolique",
            "üò¢",
            "üò≠",
            "üòî",
            "üòû",
            "üòü",
            "navr√©",
            "attrist√©",
            "d√©√ßu",
            "regret",
        ],
        "surprised": [
            "wow",
            "incroyable",
            "surprenant",
            "√©tonnant",
            "ooh",
            "waouh",
            "oh",
            "ah",
            "stup√©fait",
            "√©bahi",
            "impressionnant",
            "stup√©fiant",
            "üò≤",
            "üòÆ",
            "ü§Ø",
            "üòØ",
            "üò≥",
            "inattendu",
            "extraordinaire",
            "ahurissant",
            "attendais pas",
        ],
        "fun": [
            "dr√¥le",
            "lol",
            "mdr",
            "hilarant",
            "rigolo",
            "amusant",
            "marrant",
            "comique",
            "blague",
            "humour",
            "rire",
            "üòÜ",
            "üòÇ",
            "ü§£",
            "üòÑ",
            "haha",
            "hehe",
            "hihi",
            "comique",
            "cocasse",
            "plaisant",
        ],
        "neutral": ["ok", "bien", "voil√†", "alors", "donc", "effectivement"],
    }

    def analyze(self, text: str) -> str:
        """
        Analyse le texte et retourne l'√©motion dominante

        Args:
            text: Texte √† analyser (r√©ponse du bot)

        Returns:
            √âmotion d√©tect√©e : 'joy', 'angry', 'sorrow', 'surprised', 'fun', 'neutral'
        """
        if not text or not text.strip():
            return "neutral"

        text_lower = text.lower()

        # Compter occurrences par √©motion
        emotion_scores = {}

        for emotion, keywords in self.EMOTION_KEYWORDS.items():
            if emotion == "neutral":
                continue  # Ne pas compter neutral dans le scoring

            score = sum(1 for keyword in keywords if keyword in text_lower)

            if score > 0:
                emotion_scores[emotion] = score

        # Retourner √©motion dominante ou neutral
        if not emotion_scores:
            return "neutral"

        dominant_emotion = max(emotion_scores.items(), key=lambda x: x[1])[0]

        logger.debug(
            f"üé≠ √âmotion d√©tect√©e : {dominant_emotion} " f"(scores: {emotion_scores})"
        )

        return dominant_emotion


class ChatEngine:
    """
    Moteur conversationnel unifi√© pour Workly

    Orchestre la m√©moire, le mod√®le LLM et la d√©tection √©motionnelle
    pour g√©n√©rer des r√©ponses coh√©rentes et √©motionnelles.

    Utilisable par :
    - Interface GUI Workly (source="desktop")
    - Bot Discord (source="discord")
    """

    def __init__(
        self,
        config: Optional[AIConfig] = None,
        memory: Optional[ConversationMemory] = None,
        model_manager: Optional[ModelManager] = None,
        enable_advanced_ai: bool = False,
        memory_storage_dir: str = "data/memory",
    ):
        """
        Initialise le Chat Engine

        Args:
            config: Configuration IA (si None, charge depuis config.json)
            memory: Gestionnaire m√©moire court-terme (si None, utilise singleton)
            model_manager: Gestionnaire mod√®le (si None, utilise singleton)
            enable_advanced_ai: Active m√©moire long-terme et IA avanc√©e
            memory_storage_dir: Dossier stockage m√©moire long-terme
        """
        self.config = config or get_config()
        self.memory = memory or get_memory()
        self.model_manager = model_manager or get_model_manager(self.config)
        
        # ‚≠ê PHASE 3 : EmotionAnalyzer avanc√© (remplace EmotionDetector basique)
        # Toujours activ√© pour meilleure d√©tection √©motions (avec/sans advanced_ai)
        self.emotion_analyzer = EmotionAnalyzer(
            smoothing_factor=0.3,
            history_size=5,
            enable_emotion_memory=enable_advanced_ai  # M√©moire long-terme si IA avanc√©e
        )

        # M√©moire long-terme (Phase 1)
        self.enable_advanced_ai = enable_advanced_ai
        self.memory_manager: Optional[MemoryManager] = None
        
        # Personnalit√© √©volutive (Phase 2)
        self.personality_engine: Optional[PersonalityEngine] = None

        if enable_advanced_ai:
            # Callback LLM pour r√©sum√©s
            def llm_callback(prompt: str) -> str:
                if self.model_manager.is_loaded:
                    return self.model_manager.generate(
                        prompt=prompt,
                        temperature=0.3,  # Temp√©rature basse pour r√©sum√©s factuels
                        max_tokens=200,
                        stop=["<|user|>", "<|system|>"],
                    )
                return ""

            self.memory_manager = MemoryManager(
                storage_dir=memory_storage_dir, llm_callback=llm_callback
            )
            logger.info("‚úÖ M√©moire long-terme activ√©e (MemoryManager)")
            
            # Initialiser PersonalityEngine
            personality_file = os.path.join(memory_storage_dir, "personality.json")
            self.personality_engine = PersonalityEngine(storage_file=personality_file)
            logger.info("‚úÖ Personnalit√© √©volutive activ√©e (PersonalityEngine)")

        logger.info(
            "‚úÖ ChatEngine initialis√©"
            + (" [Mode IA Avanc√©e]" if enable_advanced_ai else "")
        )

    def _build_prompt(self, user_input: str, history: List[Dict[str, Any]]) -> str:
        """
        Construit le prompt complet avec system prompt + historique + question

        Args:
            user_input: Message actuel de l'utilisateur
            history: Historique des conversations (liste de dicts)

        Returns:
            Prompt format√© pour le mod√®le
        """
        # Format du prompt pour Zephyr-7B (format ChatML)
        prompt_parts = []

        # System prompt
        prompt_parts.append(f"<|system|>\n{self.config.system_prompt}")
        
        # ‚≠ê PHASE 2 : Injection personnalit√© (si activ√©e)
        if self.enable_advanced_ai and self.personality_engine:
            personality_prompt = self.personality_engine.generate_personality_prompt()
            prompt_parts.append(f"\n{personality_prompt}")
            logger.debug(f"üé≠ Personnalit√© inject√©e : {personality_prompt[:80]}...")

        # ‚≠ê PHASE 1 : Injection contexte long-terme (si activ√©)
        if self.enable_advanced_ai and self.memory_manager:
            long_term_context = self.memory_manager.get_context_for_prompt(
                query=user_input,
                include_facts=True,
                include_segments=True,
                max_tokens=800,  # ~20% du contexte total
            )

            if long_term_context:
                prompt_parts.append("\n--- CONTEXTE M√âMORIS√â ---")
                prompt_parts.append(long_term_context)
                prompt_parts.append("--- FIN CONTEXTE ---")
                logger.debug(
                    f"üìö Contexte long-terme inject√© : {len(long_term_context)} chars"
                )

        prompt_parts.append("</|system|>")

        # Historique des conversations (court-terme)
        for interaction in history:
            user_msg = interaction["user_input"]
            bot_msg = interaction["bot_response"]

            prompt_parts.append(f"<|user|>\n{user_msg}</|user|>")
            prompt_parts.append(f"<|assistant|>\n{bot_msg}</|assistant|>")

        # Question actuelle
        prompt_parts.append(f"<|user|>\n{user_input}</|user|>")
        prompt_parts.append("<|assistant|>")

        prompt = "\n".join(prompt_parts)

        logger.debug(
            f"üìù Prompt construit : {len(prompt)} caract√®res, "
            f"{len(history)} messages d'historique"
        )

        return prompt

    def chat(
        self, user_input: str, user_id: str = "desktop_user", source: str = "desktop"
    ) -> ChatResponse:
        """
        G√©n√®re une r√©ponse conversationnelle

        Args:
            user_input: Message de l'utilisateur
            user_id: ID utilisateur (Discord ID ou "desktop_user")
            source: Source du message ("desktop" ou "discord")

        Returns:
            ChatResponse avec r√©ponse, √©motion, stats

        Raises:
            RuntimeError: Si le mod√®le n'est pas charg√©
        """
        import time

        start_time = time.time()

        logger.info(
            f"üí¨ Chat request : user={user_id[:8]}..., "
            f"source={source}, input_len={len(user_input)}"
        )

        # V√©rifier que le mod√®le est charg√©
        if not self.model_manager.is_loaded:
            error_msg = (
                "Mod√®le LLM non charg√© ! " "Appelez model_manager.load_model() d'abord."
            )
            logger.error(f"‚ùå {error_msg}")
            raise RuntimeError(error_msg)

        # 1. Adapter personnalit√© au contexte (si activ√©e)
        if self.enable_advanced_ai and self.personality_engine:
            # D√©terminer heure du jour
            from datetime import datetime
            current_hour = datetime.now().hour
            if 5 <= current_hour < 12:
                time_of_day = 'morning'
            elif 12 <= current_hour < 18:
                time_of_day = 'afternoon'
            elif 18 <= current_hour < 22:
                time_of_day = 'evening'
            else:
                time_of_day = 'night'
            
            # R√©cup√©rer pr√©f√©rences utilisateur
            user_prefs = {}
            if self.memory_manager:
                prefs = self.memory_manager.facts.get('preferences', [])
                if prefs:
                    user_prefs['likes_humor'] = any(
                        p.get('subject') in ['humour', 'blague', 'dr√¥le'] 
                        for p in prefs if p.get('sentiment') == 'positive'
                    )
            
            # Adapter personnalit√©
            conversation_length = len(self.memory_manager.current_conversation) if self.memory_manager else 0
            self.personality_engine.adapt_to_context(
                time_of_day=time_of_day,
                conversation_length=conversation_length,
                user_preferences=user_prefs
            )

        # 2. R√©cup√©rer l'historique
        history = self.memory.get_history(
            user_id=user_id, limit=self.config.context_limit, source=source
        )

        # 3. Construire le prompt
        prompt = self._build_prompt(user_input, history)

        # 4. G√©n√©rer la r√©ponse
        try:
            response_text = self.model_manager.generate(
                prompt=prompt,
                temperature=self.config.temperature,
                top_p=self.config.top_p,
                max_tokens=self.config.max_tokens,
                stop=["<|user|>", "<|system|>"],  # Arr√™ter aux balises
            )
        except Exception as e:
            logger.error(f"‚ùå Erreur g√©n√©ration : {e}")
            raise RuntimeError(f"√âchec g√©n√©ration r√©ponse : {e}")

        # 5. Analyser l'√©motion de l'utilisateur (pour PersonalityEngine)
        user_emotion_result = self.emotion_analyzer.analyze(
            user_input,
            user_id=user_id,
            source='user'
        )
        
        # 6. Analyser l'√©motion de la r√©ponse assistant
        assistant_emotion_result = self.emotion_analyzer.analyze(
            response_text,
            user_id=user_id,
            source='assistant'
        )
        
        emotion = assistant_emotion_result.emotion  # Pour compatibilit√©
        
        # ‚≠ê PHASE 2 : Analyser feedback utilisateur (personnalit√©)
        if self.enable_advanced_ai and self.personality_engine:
            self.personality_engine.analyze_user_feedback(
                user_input,
                user_emotion=user_emotion_result.emotion
            )
        
        # ‚≠ê PHASE 3 : V√©rifier si ajustement ton n√©cessaire
        if self.enable_advanced_ai:
            tone_adjustment = self.emotion_analyzer.should_adjust_response_tone(user_id)
            if tone_adjustment:
                logger.info(f"üí° Suggestion ajustement ton : {tone_adjustment}")

        # 7. Sauvegarder l'interaction (m√©moire court-terme)
        self.memory.save_interaction(
            user_id=user_id,
            source=source,
            user_input=user_input,
            bot_response=response_text,
            emotion=emotion,
        )

        # ‚≠ê PHASE 1 : Sauvegarder dans m√©moire long-terme (si activ√©e)
        if self.enable_advanced_ai and self.memory_manager:
            # Ajouter message utilisateur
            self.memory_manager.add_message("user", user_input)

            # Ajouter r√©ponse assistant
            self.memory_manager.add_message("assistant", response_text)

            # Note : L'extraction de faits et r√©sum√©s automatiques
            # sont g√©r√©s automatiquement par MemoryManager.add_message()

        # 8. Calculer stats
        processing_time = time.time() - start_time
        tokens_used = len(response_text.split())  # Approximation

        logger.info(
            f"‚úÖ R√©ponse g√©n√©r√©e : {len(response_text)} chars, "
            f"√©motion assistant={emotion} ({assistant_emotion_result.intensity:.1f}%), "
            f"√©motion user={user_emotion_result.emotion} ({user_emotion_result.intensity:.1f}%), "
            f"temps={processing_time:.2f}s"
        )

        return ChatResponse(
            response=response_text,
            emotion=emotion,
            tokens_used=tokens_used,
            context_messages=len(history),
            processing_time=processing_time,
        )

    def clear_user_history(self, user_id: str, source: Optional[str] = None) -> int:
        """
        Efface l'historique d'un utilisateur

        Args:
            user_id: ID utilisateur
            source: Filtrer par source (optionnel)

        Returns:
            Nombre d'interactions supprim√©es
        """
        deleted = self.memory.clear_user_history(user_id, source)

        logger.info(
            f"üóëÔ∏è Historique effac√© : {deleted} interactions "
            f"pour {user_id[:8]}... (source={source or 'all'})"
        )

        return deleted

    def get_stats(self) -> Dict[str, Any]:
        """
        R√©cup√®re les statistiques globales

        Returns:
            Dictionnaire avec stats m√©moire + mod√®le + engine
        """
        memory_stats = self.memory.get_stats()
        model_info = self.model_manager.get_model_info()

        stats = {
            "memory": memory_stats,
            "model": model_info,
            "config": {
                "context_limit": self.config.context_limit,
                "gpu_profile": self.config.gpu_profile,
                "temperature": self.config.temperature,
                "max_tokens": self.config.max_tokens,
            },
        }

        # Ajouter stats m√©moire long-terme si activ√©e
        if self.enable_advanced_ai and self.memory_manager:
            stats["long_term_memory"] = self.memory_manager.get_stats()

        return stats

    def __repr__(self) -> str:
        """Repr√©sentation string du ChatEngine"""
        status = "pr√™t" if self.model_manager.is_loaded else "mod√®le non charg√©"
        return (
            f"ChatEngine({status}, "
            f"context={self.config.context_limit}, "
            f"profile={self.config.gpu_profile})"
        )


# Instance globale (optionnel, pour usage singleton)
_chat_engine_instance: Optional[ChatEngine] = None


def get_chat_engine(
    config: Optional[AIConfig] = None,
    memory: Optional[ConversationMemory] = None,
    model_manager: Optional[ModelManager] = None,
) -> ChatEngine:
    """
    R√©cup√®re l'instance globale de ChatEngine (singleton)

    Args:
        config: Configuration IA (optionnel)
        memory: Gestionnaire m√©moire (optionnel)
        model_manager: Gestionnaire mod√®le (optionnel)

    Returns:
        Instance ChatEngine
    """
    global _chat_engine_instance

    if _chat_engine_instance is None:
        _chat_engine_instance = ChatEngine(config, memory, model_manager)

    return _chat_engine_instance


# Pour tests et usage direct
if __name__ == "__main__":
    # Test rapide du ChatEngine
    print("üß™ Test du ChatEngine...\n")

    # Test 1 : Initialisation
    print("1. Initialisation ChatEngine...")
    engine = ChatEngine()
    print(f"   ‚úÖ {engine}\n")

    # Test 2 : D√©tection √©motion (EmotionAnalyzer - Phase 3)
    print("2. Test d√©tection √©motions...")
    analyzer = EmotionAnalyzer(enable_emotion_memory=False)

    tests = [
        ("Je suis super content ! üòä", "joy"),
        ("C'est vraiment triste... üò¢", "sorrow"),
        ("Wow, c'est incroyable ! üò≤", "surprised"),
        ("Haha, trop dr√¥le ! üòÇ", "fun"),
        ("Je suis tr√®s en col√®re ! üò†", "angry"),
        ("Je suis excit√© et impatient ! ü§©", "excited"),  # √âmotion compos√©e
        ("Voil√†, c'est fait.", "neutral"),
    ]

    for text, expected in tests:
        result = analyzer.analyze(text, source='user')
        emotion = result.emotion
        status = "‚úÖ" if emotion == expected else "‚ö†Ô∏è"
        print(f"   {status} '{text[:35]}...' ‚Üí {emotion} ({result.intensity:.0f}%)")

    print()

    # Test 3 : Construction prompt
    print("3. Test construction prompt...")
    history = [
        {"user_input": "Bonjour", "bot_response": "Salut !"},
        {"user_input": "√áa va ?", "bot_response": "Tr√®s bien merci !"},
    ]

    prompt = engine._build_prompt("Comment tu t'appelles ?", history)
    print(f"   ‚úÖ Prompt construit : {len(prompt)} caract√®res")
    print(f"   (Contient {prompt.count('<|user|>')} messages utilisateur)")

    print()

    # Test 4 : Chat complet (n√©cessite mod√®le charg√©)
    print("4. Test conversation compl√®te...")
    print("   ‚ö†Ô∏è N√©cessite mod√®le charg√© (d√©commentez ci-dessous)")
    print()

    # D√©commenter pour tester avec le vrai mod√®le :
    # try:
    #     engine.model_manager.load_model()
    #
    #     response = engine.chat(
    #         user_input="Bonjour Kira, pr√©sente-toi en une phrase courte.",
    #         user_id="test_user",
    #         source="desktop"
    #     )
    #
    #     print(f"   ‚úÖ R√©ponse : {response.response}")
    #     print(f"   üé≠ √âmotion : {response.emotion}")
    #     print(f"   ‚è±Ô∏è Temps : {response.processing_time:.2f}s")
    #
    #     engine.model_manager.unload_model()
    #
    # except Exception as e:
    #     print(f"   ‚ùå Erreur : {e}")

    print("‚úÖ Tests manuels termin√©s !")
